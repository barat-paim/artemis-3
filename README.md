# **LLM-Text-Classification: Fine-Tuning OPT-125M for Sentiment Analysis**  

## **Overview**  
This project fine-tunes **Metaâ€™s OPT-125M** model for **text classification**, specifically **sentiment analysis**. The pipeline is built using **PyTorch Lightning**, leveraging:  

- **Large-scale fine-tuning** on 45K training samples  
- **W&B integration** for real-time experiment tracking  
- **Early stopping & checkpointing** for stability  
- **Gradient accumulation & mixed precision** for efficient training  

This project explores **LLM-based text classification performance and scalability** with real-world datasets.  

---

## **ğŸš€ Key Features**  
âœ… **Fine-tunes OPT-125M** on a large-scale sentiment dataset  
âœ… **PyTorch Lightning pipeline** for modular training  
âœ… **W&B logging** for tracking loss, accuracy, and model checkpoints  
âœ… **Early stopping** to prevent overfitting  
âœ… **Gradient accumulation** for handling large batches on limited GPU memory  
âœ… **Multi-GPU training support**  

---

## **ğŸ“ Project Structure**  

```bash
â”œâ”€â”€ llm-text-classification/
â”‚   â”œâ”€â”€ main.py                   # Training pipeline
â”‚   â”œâ”€â”€ config.py                 # Training hyperparameters
â”‚   â”œâ”€â”€ dataloader.py             # Dataset loading module
â”‚   â”œâ”€â”€ lightning_trainer.py      # PyTorch Lightning trainer
â”‚   â”œâ”€â”€ lightning_model_utils.py  # Model setup utilities
â”‚   â”œâ”€â”€ lightning_inference.py    # Model inference module
â”‚   â”œâ”€â”€ README.md                 # Project documentation (this file)
```

---

## **ğŸ› ï¸ Setup & Training**  

### **1ï¸âƒ£ Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **2ï¸âƒ£ Run Fine-Tuning**  
```bash
python main.py
```

ğŸ’¡ **To train on a multi-GPU setup:**  
Modify `config.py` to use **multiple devices** and increase `batch_size`.  

---

## **ğŸ“Š Evaluation & Results**  

| Model  | Dataset  | Accuracy | F1 Score |  
|--------|---------|----------|----------|  
| OPT-125M | Custom Sentiment Dataset | **85%** | **0.87** |  

âœ”ï¸ **Final Performance:** Achieved **85% accuracy** on sentiment classification  
âœ”ï¸ **Challenges:** Handling **large-scale fine-tuning on limited GPU memory**  
âœ”ï¸ **Solution:** Used **gradient accumulation & mixed precision**  

---

## **ğŸ” Inference (Classify Sentiment)**  

```bash
python lightning_inference.py --text "This movie was amazing!"
```
Example Output:  
```json
{
  "text": "This movie was amazing!",
  "predicted_label": "positive"
}
```

---

## **ğŸ”® Next Steps**  
ğŸ”¹ **Experiment with larger models (OPT-350M, OPT-1.3B)**  
ğŸ”¹ **Expand to multi-class classification (e.g., emotions, topics)**  
ğŸ”¹ **Deploy as an API for real-time classification**  

---

## **ğŸ“Œ Why This Project Matters?**  
This project **demonstrates LLM fine-tuning for text classification**, solving:  
- **Handling large-scale datasets efficiently**  
- **Real-time experiment tracking with W&B**  
- **Optimized training on resource-constrained GPUs**  

ğŸ“Œ **Ideal for:** AI/ML roles focusing on **LLM fine-tuning, model evaluation, and scalable NLP applications.**  
